# Priority Queue vs End-of-Process Sorting in Async I/O Processing

## Why Priority Queue Over End-Sorting?

Using a priority queue (`heapq`) during processing instead of sorting at the end offers several advantages:

1. **Streaming Results**: You can begin outputting results as soon as chunks complete, rather than waiting for all processing to finish[1][2].

2. **Memory Efficiency**: With a priority queue, you only maintain ordering metadata in memory rather than buffering all results[1][2].

3. **Partial Processing**: If the process is interrupted, you still have partial ordered results rather than losing everything[1].

4. **Real-time Progress**: You can provide immediate feedback on completed chunks in proper order[2].

However, there's an important consideration: **async tasks complete in unpredictable order**. A priority queue helps maintain the correct file-order sequence as results arrive out-of-order.

***

# Software Implementation: Async I/O + mmap + Priority Queue File Processor

## Architecture Overview

This implementation combines three performance techniques:
- **Memory-mapped I/O**: Zero-copy file access with kernel-optimized read-ahead
- **Async I/O**: Non-blocking concurrency for overlapping prefetch and processing  
- **Priority Queue**: Maintains file-order output while processing chunks concurrently

## Core Components

### 1. Priority Queue Result Manager

```python
import heapq
import asyncio
from dataclasses import dataclass
from typing import List, Any

@dataclass
class ProcessedChunk:
    """Container for processed chunk with ordering metadata"""
    offset: int
    length: int  
    results: List[Any]
    
    def __lt__(self, other):
        """Enable heapq ordering by file offset"""
        return self.offset < other.offset

class OrderedResultManager:
    """Manages async results in file-order using priority queue"""
    
    def __init__(self):
        self._heap = []
        self._next_expected_offset = 0
        self._output_queue = asyncio.Queue()
        
    async def add_result(self, chunk: ProcessedChunk):
        """Add completed chunk to priority queue"""
        heapq.heappush(self._heap, chunk)
        await self._check_and_output_ready()
        
    async def _check_and_output_ready(self):
        """Output any sequential chunks starting from expected offset"""
        while (self._heap and 
               self._heap[0].offset == self._next_expected_offset):
            ready_chunk = heapq.heappop(self._heap)
            await self._output_queue.put(ready_chunk)
            self._next_expected_offset += ready_chunk.length
            
    async def get_next_result(self):
        """Get next result in file order"""
        return await self._output_queue.get()
        
    def has_pending(self):
        """Check if results are pending output"""
        return len(self._heap) > 0 or not self._output_queue.empty()
```

### 2. Async Memory-Mapped Prefetcher

```python
import mmap
import os

PAGE_SIZE = mmap.PAGESIZE
PREFETCH_STRIDE = PAGE_SIZE * 16
CHUNK_SIZE = 64 * 1024 * 1024  # 64 MiB

class AsyncMemoryMappedReader:
    """Async wrapper for memory-mapped file reading with prefetch"""
    
    def __init__(self, filepath: str):
        self.filepath = filepath
        self.file_size = os.path.getsize(filepath)
        self._fd = None
        
    async def __aenter__(self):
        self._file = open(self.filepath, 'rb')
        self._fd = self._file.fileno()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._file:
            self._file.close()
            
    async def prefetch_chunk(self, offset: int, length: int):
        """Async prefetch of memory-mapped region"""
        mm = mmap.mmap(self._fd, length, access=mmap.ACCESS_READ, offset=offset)
        try:
            # Touch pages in strides to trigger kernel read-ahead
            end = min(length, len(mm))
            for pos in range(0, end, PREFETCH_STRIDE):
                _ = mm[pos]  # Trigger page fault
                await asyncio.sleep(0)  # Yield to event loop
            return mm
        except Exception:
            mm.close()
            raise
            
    def get_chunk_boundaries(self):
        """Generate (offset, length) tuples for file chunks"""
        for offset in range(0, self.file_size, CHUNK_SIZE):
            length = min(CHUNK_SIZE, self.file_size - offset)
            yield offset, length
```

### 3. Line Processing Worker

```python
async def process_chunk_lines(mm: mmap.mmap, offset: int, length: int) -> ProcessedChunk:
    """Process lines within a memory-mapped chunk"""
    try:
        # Extract and split data, handling incomplete lines at boundaries
        data = mm[:length]
        lines = data.split(b'\n')
        
        # Process each complete line
        results = []
        for line_data in lines:
            if not line_data:  # Skip empty lines
                continue
                
            try:
                line = line_data.decode('utf-8', errors='ignore')
                # Example processing: word count and character analysis
                word_count = len(line.split())
                char_count = len(line)
                line_result = {
                    'words': word_count,
                    'chars': char_count,
                    'has_numbers': any(c.isdigit() for c in line)
                }
                results.append(line_result)
                
                # Yield periodically for async responsiveness
                if len(results) % 100 == 0:
                    await asyncio.sleep(0)
                    
            except UnicodeDecodeError:
                continue  # Skip problematic lines
                
        return ProcessedChunk(offset=offset, length=length, results=results)
        
    finally:
        mm.close()
```

### 4. Main Orchestration Engine

```python
async def async_file_processor(filepath: str, max_concurrent_chunks: int = 8):
    """Main processing engine combining all components"""
    
    result_manager = OrderedResultManager()
    final_dataset = []
    
    async with AsyncMemoryMappedReader(filepath) as reader:
        # Create semaphore to limit concurrent chunks
        semaphore = asyncio.Semaphore(max_concurrent_chunks)
        
        async def process_single_chunk(offset: int, length: int):
            """Process one chunk with concurrency control"""
            async with semaphore:
                # Prefetch and map the chunk
                mm = await reader.prefetch_chunk(offset, length)
                
                # Process the chunk
                processed_chunk = await process_chunk_lines(mm, offset, length)
                
                # Add to ordered results
                await result_manager.add_result(processed_chunk)
        
        # Launch all chunk processing tasks
        chunk_tasks = []
        for offset, length in reader.get_chunk_boundaries():
            task = asyncio.create_task(process_single_chunk(offset, length))
            chunk_tasks.append(task)
            
        # Consumer coroutine to collect ordered results
        async def result_collector():
            """Collect results in file order as they become available"""
            chunks_processed = 0
            total_chunks = len(chunk_tasks)
            
            while chunks_processed < total_chunks:
                chunk = await result_manager.get_next_result()
                final_dataset.extend(chunk.results)
                chunks_processed += 1
                
                # Optional: Progress reporting
                if chunks_processed % 10 == 0:
                    print(f"Processed {chunks_processed}/{total_chunks} chunks")
        
        # Start result collection
        collector_task = asyncio.create_task(result_collector())
        
        # Wait for all processing to complete
        await asyncio.gather(*chunk_tasks)
        await collector_task
        
    return final_dataset

# Example usage
async def main():
    dataset = await async_file_processor('large_text_file.txt', max_concurrent_chunks=12)
    print(f"Processed {len(dataset)} lines")
    print("Sample results:", dataset[:5])

if __name__ == "__main__":
    asyncio.run(main())
```

## Performance Benefits

1. **Overlapped I/O**: While some chunks are being read from disk, others are being processed in memory[3][4].

2. **Zero-Copy Access**: `mmap` eliminates buffer copying between kernel and user space[5].

3. **Ordered Output**: Priority queue ensures results maintain file order despite async completion[1][2].

4. **Memory Efficiency**: Only active chunks consume memory; processed chunks are immediately available for output[6].

5. **Scalable Concurrency**: Semaphore prevents memory exhaustion while maximizing throughput[7][3].

## Configuration Parameters

- **`CHUNK_SIZE`**: Larger chunks reduce overhead but increase memory usage
- **`PREFETCH_STRIDE`**: Balance between read-ahead aggressiveness and CPU overhead  
- **`max_concurrent_chunks`**: Tune based on available memory and I/O capacity
- **Yield frequency**: Control responsiveness vs processing throughput in tight loops

This architecture delivers high-performance file processing by efficiently combining async I/O concurrency with memory-mapped zero-copy access, while maintaining correct output ordering through priority queue management.

Citations:
[1] Implementing a Priority Queue in Python: A Comprehensive Guide https://hostman.com/tutorials/implementing-a-priority-queue-in-python/
[2] Heap and Priority Queue using heapq module in Python https://www.geeksforgeeks.org/python/heap-and-priority-queue-using-heapq-module-in-python/
[3] Unleashing the Power of Python Asyncio's Queue - Data Leads Future https://www.dataleadsfuture.com/unleashing-the-power-of-python-asyncios-queue/
[4] Producer-consumer simulation with asyncio queues in Python https://www.w3resource.com/python-exercises/asynchronous/python-asynchronous-exercise-8.php
[5] mmap — Memory-mapped file support — Python 3.13.7 ... https://docs.python.org/3/library/mmap.html
[6] How to Use a Priority Queue in Python - DigitalOcean https://www.digitalocean.com/community/tutorials/priority-queue-python
[7] Producer-consumer patterns with asyncio - StudyRaid https://app.studyraid.com/en/read/15007/518816/producer-consumer-patterns-with-asyncio
[8] mmap writes to file on disk(synchronous/asynchronous) https://stackoverflow.com/questions/2226740/mmap-writes-to-file-on-disksynchronous-asynchronous
[9] Is mmap + madvise really a form of async I/O? - Stack Overflow https://stackoverflow.com/questions/31215250/is-mmap-madvise-really-a-form-of-async-i-o
[10] Async file IO using memmap - Rust Users Forum https://users.rust-lang.org/t/async-file-io-using-memmap/67606
[11] Implementing Priority Queue in Python with Heapq https://python.plainenglish.io/implementing-priority-queue-in-python-with-heapq-168d084f179d
[12] Understanding producer-consumer program with asyncio https://stackoverflow.com/questions/71568584/understanding-producer-consumer-program-with-asyncio
[13] Async hazard: MMAP is blocking IO | Hacker News https://news.ycombinator.com/item?id=41312124
[14] Heap queue or heapq in Python - GeeksforGeeks https://www.geeksforgeeks.org/python/heap-queue-or-heapq-in-python/
[15] Introduction to Priority Queues in Python | Built In https://builtin.com/data-science/priority-queues-in-python
